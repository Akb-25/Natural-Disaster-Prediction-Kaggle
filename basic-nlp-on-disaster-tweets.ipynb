{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Basic NLP on Disaster Tweets\n","\n","This is a getting started competition of predicting whether a tweet is actually referring to real disasters."]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\25bak\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]},{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/nlp-getting-started/train.csv'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22752\\3821010096.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Load data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/kaggle/input/nlp-getting-started/train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/kaggle/input/nlp-getting-started/test.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0msub_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/kaggle/input/nlp-getting-started/sample_submission.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\25bak\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\25bak\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\25bak\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\25bak\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\25bak\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\25bak\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/nlp-getting-started/train.csv'"]}],"source":["# Import libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import string\n","\n","plt.rcParams.update({'font.size': 14})\n","\n","# Load data\n","train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n","test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n","sub_sample = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n","\n","print (train.shape, test.shape, sub_sample.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Basic Exploration"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train.duplicated().sum()"]},{"cell_type":"markdown","metadata":{},"source":["There are 52 duplicated rows. The duplicates will be removed."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train = train.drop_duplicates().reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Class balance\n","# train.target.value_counts()\n","sns.countplot(y=train.target);"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# NA data\n","train.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test.isnull().sum()"]},{"cell_type":"markdown","metadata":{},"source":["Text is all non-null. Only a small percentage of tweets have no keyword. Location has much more null values."]},{"cell_type":"markdown","metadata":{},"source":["## 2. Keywords"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Check number of unique keywords, and whether they are the same for train and test sets\n","print (train.keyword.nunique(), test.keyword.nunique())\n","print (set(train.keyword.unique()) - set(test.keyword.unique()))"]},{"cell_type":"markdown","metadata":{},"source":["Train and test have the same set of keywords"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Most common keywords\n","plt.figure(figsize=(9,6))\n","sns.countplot(y=train.keyword, order = train.keyword.value_counts().iloc[:15].index)\n","plt.title('Top 15 keywords')\n","plt.show()\n","# train.keyword.value_counts().head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["kw_d = train[train.target==1].keyword.value_counts().head(10)\n","kw_nd = train[train.target==0].keyword.value_counts().head(10)\n","\n","plt.figure(figsize=(13,5))\n","plt.subplot(121)\n","sns.barplot(kw_d, kw_d.index, color='c')\n","plt.title('Top keywords for disaster tweets')\n","plt.subplot(122)\n","sns.barplot(kw_nd, kw_nd.index, color='y')\n","plt.title('Top keywords for non-disaster tweets')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["There is no common top 10 keywords between disaster and non-disaster tweets."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["top_d = train.groupby('keyword').mean()['target'].sort_values(ascending=False).head(10)\n","top_nd = train.groupby('keyword').mean()['target'].sort_values().head(10)\n","\n","plt.figure(figsize=(13,5))\n","plt.subplot(121)\n","sns.barplot(top_d, top_d.index, color='pink')\n","plt.title('Keywords with highest % of disaster tweets')\n","plt.subplot(122)\n","sns.barplot(top_nd, top_nd.index, color='yellow')\n","plt.title('Keywords with lowest % of disaster tweets')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Locations"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Check number of unique keywords and locations\n","print (train.location.nunique(), test.location.nunique())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Most common locations\n","plt.figure(figsize=(9,6))\n","sns.countplot(y=train.location, order = train.location.value_counts().iloc[:15].index)\n","plt.title('Top 15 locations')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["As location is free text, the data is not clean, you can see both 'USA' and 'United States' in top locations. We than have a look at % of disaster tweets for common locations."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["raw_loc = train.location.value_counts()\n","top_loc = list(raw_loc[raw_loc>=10].index)\n","top_only = train[train.location.isin(top_loc)]\n","\n","top_l = top_only.groupby('location').mean()['target'].sort_values(ascending=False)\n","plt.figure(figsize=(14,6))\n","sns.barplot(x=top_l.index, y=top_l)\n","plt.axhline(np.mean(train.target))\n","plt.xticks(rotation=80)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["The top 3 locations with highest % of disaster tweets are **Mumbai, Inida, and Nigeria**. As the location data is not clean, we see some interesting cases, such as **'London, UK' saw a higher-than-average % of disaster tweets, but 'London' is below average**. We try to clean up the location and see if there is any difference:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["# Fill NA values\n","for col in ['keyword','location']:\n","    train[col] = train[col].fillna('None')\n","    test[col] = test[col].fillna('None')\n","\n","def clean_loc(x):\n","    if x == 'None':\n","        return 'None'\n","    elif x == 'Earth' or x =='Worldwide' or x == 'Everywhere':\n","        return 'World'\n","    elif 'New York' in x or 'NYC' in x:\n","        return 'New York'    \n","    elif 'London' in x:\n","        return 'London'\n","    elif 'Mumbai' in x:\n","        return 'Mumbai'\n","    elif 'Washington' in x and 'D' in x and 'C' in x:\n","        return 'Washington DC'\n","    elif 'San Francisco' in x:\n","        return 'San Francisco'\n","    elif 'Los Angeles' in x:\n","        return 'Los Angeles'\n","    elif 'Seattle' in x:\n","        return 'Seattle'\n","    elif 'Chicago' in x:\n","        return 'Chicago'\n","    elif 'Toronto' in x:\n","        return 'Toronto'\n","    elif 'Sacramento' in x:\n","        return 'Sacramento'\n","    elif 'Atlanta' in x:\n","        return 'Atlanta'\n","    elif 'California' in x:\n","        return 'California'\n","    elif 'Florida' in x:\n","        return 'Florida'\n","    elif 'Texas' in x:\n","        return 'Texas'\n","    elif 'United States' in x or 'USA' in x:\n","        return 'USA'\n","    elif 'United Kingdom' in x or 'UK' in x or 'Britain' in x:\n","        return 'UK'\n","    elif 'Canada' in x:\n","        return 'Canada'\n","    elif 'India' in x:\n","        return 'India'\n","    elif 'Kenya' in x:\n","        return 'Kenya'\n","    elif 'Nigeria' in x:\n","        return 'Nigeria'\n","    elif 'Australia' in x:\n","        return 'Australia'\n","    elif 'Indonesia' in x:\n","        return 'Indonesia'\n","    elif x in top_loc:\n","        return x\n","    else: return 'Others'\n","    \n","train['location_clean'] = train['location'].apply(lambda x: clean_loc(str(x)))\n","test['location_clean'] = test['location'].apply(lambda x: clean_loc(str(x)))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["top_l2 = train.groupby('location_clean').mean()['target'].sort_values(ascending=False)\n","plt.figure(figsize=(14,6))\n","sns.barplot(x=top_l2.index, y=top_l2)\n","plt.axhline(np.mean(train.target))\n","plt.xticks(rotation=80)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Mumbai and Nigeria are still on the top. Other than the strange 'ss', London and New York made the bottom of % of disaster tweets."]},{"cell_type":"markdown","metadata":{},"source":["## 4. Spoiler Alert!\n","\n","(Spoiler alert) You Can Get Perfect Score in (public) Leader Board!\n","\n","You will see in the public leaderboard, many participants got a perfect score. It is because the whole dataset with label is available online (one copy available [on Kaggle](https://www.kaggle.com/jannesklaas/disasters-on-social-media)). You can find the correct label of our test set so you can achieve perfect score.\n","\n","In such case, the ranking on public leaderboard is meaningless. The good news is, you can now focus on learning NLP and modelling skills with this dataset, instead of fighting for higher position on the leaderboard!\n","\n","(Reference: [szelee's notebook](https://www.kaggle.com/szelee/a-real-disaster-leaked-label/notebook))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"outputs":[],"source":["leak = pd.read_csv(\"../input/disasters-on-social-media/socialmedia-disaster-tweets-DFE.csv\", encoding='latin_1')\n","leak['target'] = (leak['choose_one']=='Relevant').astype(int)\n","leak['id'] = leak.index\n","leak = leak[['id', 'target','text']]\n","merged_df = pd.merge(test, leak, on='id')\n","sub1 = merged_df[['id', 'target']]\n","sub1.to_csv('submit_1.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Clean up Text Column\n","\n","Here we clean up the text column by:\n","- Making a 'clean' text column, removing links and unnecessary white spaces\n","- Creating separate columns containing lists of hashtags, mentions, and links"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import re\n","\n","test_str = train.loc[417, 'text']\n","\n","def clean_text(text):\n","    text = re.sub(r'https?://\\S+', '', text) # Remove link\n","    text = re.sub(r'\\n',' ', text) # Remove line breaks\n","    text = re.sub('\\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces\n","    return text\n","\n","print(\"Original text: \" + test_str)\n","print(\"Cleaned text: \" + clean_text(test_str))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def find_hashtags(tweet):\n","    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or 'no'\n","\n","def find_mentions(tweet):\n","    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"@\\w+\", tweet)]) or 'no'\n","\n","def find_links(tweet):\n","    return \" \".join([match.group(0)[:] for match in re.finditer(r\"https?://\\S+\", tweet)]) or 'no'\n","\n","def process_text(df):\n","    \n","    df['text_clean'] = df['text'].apply(lambda x: clean_text(x))\n","    df['hashtags'] = df['text'].apply(lambda x: find_hashtags(x))\n","    df['mentions'] = df['text'].apply(lambda x: find_mentions(x))\n","    df['links'] = df['text'].apply(lambda x: find_links(x))\n","    # df['hashtags'].fillna(value='no', inplace=True)\n","    # df['mentions'].fillna(value='no', inplace=True)\n","    \n","    return df\n","    \n","train = process_text(train)\n","test = process_text(test)"]},{"cell_type":"markdown","metadata":{},"source":["## 6. Create statistics from texts"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from wordcloud import STOPWORDS\n","\n","def create_stat(df):\n","    # Tweet length\n","    df['text_len'] = df['text_clean'].apply(len)\n","    # Word count\n","    df['word_count'] = df[\"text_clean\"].apply(lambda x: len(str(x).split()))\n","    # Stopword count\n","    df['stop_word_count'] = df['text_clean'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n","    # Punctuation count\n","    df['punctuation_count'] = df['text_clean'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n","    # Count of hashtags (#)\n","    df['hashtag_count'] = df['hashtags'].apply(lambda x: len(str(x).split()))\n","    # Count of mentions (@)\n","    df['mention_count'] = df['mentions'].apply(lambda x: len(str(x).split()))\n","    # Count of links\n","    df['link_count'] = df['links'].apply(lambda x: len(str(x).split()))\n","    # Count of uppercase letters\n","    df['caps_count'] = df['text_clean'].apply(lambda x: sum(1 for c in str(x) if c.isupper()))\n","    # Ratio of uppercase letters\n","    df['caps_ratio'] = df['caps_count'] / df['text_len']\n","    return df\n","\n","train = create_stat(train)\n","test = create_stat(test)\n","\n","print(train.shape, test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train.corr()['target'].drop('target').sort_values()"]},{"cell_type":"markdown","metadata":{},"source":["All of the statistics have very low correlation with the target variable"]},{"cell_type":"markdown","metadata":{},"source":["## 7. Most frequent words and bigrams\n","\n","What are the most common unigrams (single word) and bigrams (two-word sequence)?"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from nltk import FreqDist, word_tokenize\n","\n","# Make a set of stop words\n","stopwords = set(STOPWORDS)\n","# more_stopwords = {'https', 'amp'}\n","# stopwords = stopwords.union(more_stopwords)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Unigrams\n","word_freq = FreqDist(w for w in word_tokenize(' '.join(train['text_clean']).lower()) if \n","                     (w not in stopwords) & (w.isalpha()))\n","df_word_freq = pd.DataFrame.from_dict(word_freq, orient='index', columns=['count'])\n","top20w = df_word_freq.sort_values('count',ascending=False).head(20)\n","\n","plt.figure(figsize=(8,6))\n","sns.barplot(top20w['count'], top20w.index)\n","plt.title('Top 20 words')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We try to distinguish disaster and non-disaster tweets:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(16,7))\n","plt.subplot(121)\n","freq_d = FreqDist(w for w in word_tokenize(' '.join(train.loc[train.target==1, 'text_clean']).lower()) if \n","                     (w not in stopwords) & (w.isalpha()))\n","df_d = pd.DataFrame.from_dict(freq_d, orient='index', columns=['count'])\n","top20_d = df_d.sort_values('count',ascending=False).head(20)\n","sns.barplot(top20_d['count'], top20_d.index, color='c')\n","plt.title('Top words in disaster tweets')\n","plt.subplot(122)\n","freq_nd = FreqDist(w for w in word_tokenize(' '.join(train.loc[train.target==0, 'text_clean']).lower()) if \n","                     (w not in stopwords) & (w.isalpha()))\n","df_nd = pd.DataFrame.from_dict(freq_nd, orient='index', columns=['count'])\n","top20_nd = df_nd.sort_values('count',ascending=False).head(20)\n","sns.barplot(top20_nd['count'], top20_nd.index, color='y')\n","plt.title('Top words in non-disaster tweets')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Findings:\n","- Top two words in disaster tweets: 'fire' and 'news', don't make the top 20 on unreal disaster tweets.\n","- Words are more specific for real disaster tweets (e.g. 'califonia', 'hiroshima', 'fire', 'police', 'suicide', 'bomb')."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Bigrams\n","\n","from nltk import bigrams\n","\n","plt.figure(figsize=(16,7))\n","plt.subplot(121)\n","bigram_d = list(bigrams([w for w in word_tokenize(' '.join(train.loc[train.target==1, 'text_clean']).lower()) if \n","              (w not in stopwords) & (w.isalpha())]))\n","d_fq = FreqDist(bg for bg in bigram_d)\n","bgdf_d = pd.DataFrame.from_dict(d_fq, orient='index', columns=['count'])\n","bgdf_d.index = bgdf_d.index.map(lambda x: ' '.join(x))\n","bgdf_d = bgdf_d.sort_values('count',ascending=False)\n","sns.barplot(bgdf_d.head(20)['count'], bgdf_d.index[:20], color='pink')\n","plt.title('Top bigrams in disaster tweets')\n","plt.subplot(122)\n","bigram_nd = list(bigrams([w for w in word_tokenize(' '.join(train.loc[train.target==0, 'text_clean']).lower()) if \n","              (w not in stopwords) & (w.isalpha())]))\n","nd_fq = FreqDist(bg for bg in bigram_nd)\n","bgdf_nd = pd.DataFrame.from_dict(nd_fq, orient='index', columns=['count'])\n","bgdf_nd.index = bgdf_nd.index.map(lambda x: ' '.join(x))\n","bgdf_nd = bgdf_nd.sort_values('count',ascending=False)\n","sns.barplot(bgdf_nd.head(20)['count'], bgdf_nd.index[:20], color='yellow')\n","plt.title('Top bigrams in non-disaster tweets')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Findings:\n","- Most top bigrams in disaster tweets show certain kinds of catestrophe (e.g. suicide bomber, oil spill, california wildfire); for non-disaster tweets, only 'burning buildings' as top bigram look like a disaster;\n","- Top bigrams in disaster tweets have a more casual tone;\n","- 'youtube' appears in three of the twenty bigrams for non-disaster tweets; none in disaster tweets"]},{"cell_type":"markdown","metadata":{},"source":["## 8. Encoding and Vectorizers\n","\n","As part of feature generation, we will:\n","- Apply target encoding to keyword and location (cleaned)\n","- Count Vectorize cleaned text, links, hashtags and mentions columns"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import category_encoders as ce\n","\n","# Target encoding\n","features = ['keyword', 'location_clean']\n","encoder = ce.TargetEncoder(cols=features)\n","encoder.fit(train[features],train['target'])\n","\n","train = train.join(encoder.transform(train[features]).add_suffix('_target'))\n","test = test.join(encoder.transform(test[features]).add_suffix('_target'))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# CountVectorizer\n","\n","# Links\n","vec_links = CountVectorizer(min_df = 5, analyzer = 'word', token_pattern = r'https?://\\S+') # Only include those >=5 occurrences\n","link_vec = vec_links.fit_transform(train['links'])\n","link_vec_test = vec_links.transform(test['links'])\n","X_train_link = pd.DataFrame(link_vec.toarray(), columns=vec_links.get_feature_names())\n","X_test_link = pd.DataFrame(link_vec_test.toarray(), columns=vec_links.get_feature_names())\n","\n","# Mentions\n","vec_men = CountVectorizer(min_df = 5)\n","men_vec = vec_men.fit_transform(train['mentions'])\n","men_vec_test = vec_men.transform(test['mentions'])\n","X_train_men = pd.DataFrame(men_vec.toarray(), columns=vec_men.get_feature_names())\n","X_test_men = pd.DataFrame(men_vec_test.toarray(), columns=vec_men.get_feature_names())\n","\n","# Hashtags\n","vec_hash = CountVectorizer(min_df = 5)\n","hash_vec = vec_hash.fit_transform(train['hashtags'])\n","hash_vec_test = vec_hash.transform(test['hashtags'])\n","X_train_hash = pd.DataFrame(hash_vec.toarray(), columns=vec_hash.get_feature_names())\n","X_test_hash = pd.DataFrame(hash_vec_test.toarray(), columns=vec_hash.get_feature_names())\n","print (X_train_link.shape, X_train_men.shape, X_train_hash.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["_ = (X_train_link.transpose().dot(train['target']) / X_train_link.sum(axis=0)).sort_values(ascending=False)\n","plt.figure(figsize=(10,6))\n","sns.barplot(x=_, y=_.index)\n","plt.axvline(np.mean(train.target))\n","plt.title('% of disaster tweet given links')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["_ = (X_train_men.transpose().dot(train['target']) / X_train_men.sum(axis=0)).sort_values(ascending=False)\n","plt.figure(figsize=(14,6))\n","sns.barplot(x=_.index, y=_)\n","plt.axhline(np.mean(train.target))\n","plt.title('% of disaster tweet given mentions')\n","plt.xticks(rotation = 50)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["hash_rank = (X_train_hash.transpose().dot(train['target']) / X_train_hash.sum(axis=0)).sort_values(ascending=False)\n","print('Hashtags with which 100% of Tweets are disasters: ')\n","print(list(hash_rank[hash_rank==1].index))\n","print('Total: ' + str(len(hash_rank[hash_rank==1])))\n","print('Hashtags with which 0% of Tweets are disasters: ')\n","print(list(hash_rank[hash_rank==0].index))\n","print('Total: ' + str(len(hash_rank[hash_rank==0])))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Tf-idf for text\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vec_text = TfidfVectorizer(min_df = 10, ngram_range = (1,2), stop_words='english') \n","# Only include >=10 occurrences\n","# Have unigrams and bigrams\n","text_vec = vec_text.fit_transform(train['text_clean'])\n","text_vec_test = vec_text.transform(test['text_clean'])\n","X_train_text = pd.DataFrame(text_vec.toarray(), columns=vec_text.get_feature_names())\n","X_test_text = pd.DataFrame(text_vec_test.toarray(), columns=vec_text.get_feature_names())\n","print (X_train_text.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Joining the dataframes together\n","\n","train = train.join(X_train_link, rsuffix='_link')\n","train = train.join(X_train_men, rsuffix='_mention')\n","train = train.join(X_train_hash, rsuffix='_hashtag')\n","train = train.join(X_train_text, rsuffix='_text')\n","test = test.join(X_test_link, rsuffix='_link')\n","test = test.join(X_test_men, rsuffix='_mention')\n","test = test.join(X_test_hash, rsuffix='_hashtag')\n","test = test.join(X_test_text, rsuffix='_text')\n","print (train.shape, test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## 9. Simple Model: Logistic Regression\n","\n","We try the simplest model with logistic regression, based on all features we created above. Before we fit a model, we first transform the features into the same scale with minimum 0 and maximum 1. We do this in the form of pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import MinMaxScaler\n","\n","features_to_drop = ['id', 'keyword','location','text','location_clean','text_clean', 'hashtags', 'mentions','links']\n","scaler = MinMaxScaler()\n","\n","X_train = train.drop(columns = features_to_drop + ['target'])\n","X_test = test.drop(columns = features_to_drop)\n","y_train = train.target\n","\n","lr = LogisticRegression(solver='liblinear', random_state=777) # Other solvers have failure to converge problem\n","\n","pipeline = Pipeline([('scale',scaler), ('lr', lr),])\n","\n","pipeline.fit(X_train, y_train)\n","y_test = pipeline.predict(X_test)\n","\n","submit = sub_sample.copy()\n","submit.target = y_test\n","submit.to_csv('submit_lr.csv',index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print ('Training accuracy: %.4f' % pipeline.score(X_train, y_train))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# F-1 score\n","from sklearn.metrics import f1_score\n","\n","print ('Training f-1 score: %.4f' % f1_score(y_train, pipeline.predict(X_train)))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Confusion matrix\n","from sklearn.metrics import confusion_matrix\n","pd.DataFrame(confusion_matrix(y_train, pipeline.predict(X_train)))"]},{"cell_type":"markdown","metadata":{},"source":["This forms the basis of evaluating and modifying our models in the next section."]},{"cell_type":"markdown","metadata":{},"source":["## 10. Evaluate and Improve Our Model\n","\n","Several things will be done:\n","- Cross validation with shuffle split\n","- Feature selections\n","- Grid search for hyperparameters\n","- Identify errors\n","\n","Reference: [scoring parameters](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Cross validation\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import ShuffleSplit\n","\n","cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=123)\n","cv_score = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='f1')\n","print('Cross validation F-1 score: %.3f' %np.mean(cv_score))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Top features\n","plt.figure(figsize=(16,7))\n","s1 = pd.Series(np.transpose(lr.coef_[0]), index=X_train.columns).sort_values(ascending=False)[:20]\n","s2 = pd.Series(np.transpose(lr.coef_[0]), index=X_train.columns).sort_values()[:20]\n","plt.subplot(121)\n","sns.barplot(y=s1.index, x=s1)\n","plt.title('Top positive coefficients')\n","plt.subplot(122)\n","sns.barplot(y=s2.index, x=s2)\n","plt.title('Top negative coefficients')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Findings:\n","- 'keyword_target' is the top positive coefficient, meaning the keyword column made a good feature\n","- hiroshima both as text and hashtag made the top 20 positive coefficients\n","- Punctuation count and stop word count are among top 20 negative coefficients\n","- None of the bigrams made the top features"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Feature selection\n","from sklearn.feature_selection import RFECV\n","\n","steps = 20\n","n_features = len(X_train.columns)\n","X_range = np.arange(n_features - (int(n_features/steps)) * steps, n_features+1, steps)\n","\n","rfecv = RFECV(estimator=lr, step=steps, cv=cv, scoring='f1')\n","\n","pipeline2 = Pipeline([('scale',scaler), ('rfecv', rfecv)])\n","pipeline2.fit(X_train, y_train)\n","plt.figure(figsize=(10,6))\n","plt.xlabel(\"Number of features selected\")\n","plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n","plt.plot(np.insert(X_range, 0, 1), rfecv.grid_scores_)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print ('Optimal no. of features: %d' % np.insert(X_range, 0, 1)[np.argmax(rfecv.grid_scores_)])"]},{"cell_type":"markdown","metadata":{},"source":["We then pick up the 1133 selected features to do Grid Search CV to find optimal hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["selected_features = X_train.columns[rfecv.ranking_ == 1]\n","X_train2 = X_train[selected_features]\n","X_test2 = X_test[selected_features]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# lr2 = LogisticRegression(solver='liblinear', random_state=37)\n","pipeline.fit(X_train2, y_train)\n","cv2 = ShuffleSplit(n_splits=5, test_size=0.2, random_state=456)\n","cv_score2 = cross_val_score(pipeline, X_train2, y_train, cv=cv2, scoring='f1')\n","print('Cross validation F-1 score: %.3f' %np.mean(cv_score2))"]},{"cell_type":"markdown","metadata":{},"source":["A little improvement from the earlier model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","grid={\"C\":np.logspace(-2,2,5), \"penalty\":[\"l1\",\"l2\"]}\n","lr_cv = GridSearchCV(LogisticRegression(solver='liblinear', random_state=20), grid, cv=cv2, scoring = 'f1')\n","\n","pipeline_grid = Pipeline([('scale',scaler), ('gridsearch', lr_cv),])\n","\n","pipeline_grid.fit(X_train2, y_train)\n","\n","print(\"Best parameter: \", lr_cv.best_params_)\n","print(\"F-1 score: %.3f\" %lr_cv.best_score_)"]},{"cell_type":"markdown","metadata":{},"source":["Default hyperparameters gave the best score."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Submit fine-tuned model\n","\n","y_test2 = pipeline_grid.predict(X_test2)\n","submit2 = sub_sample.copy()\n","submit2.target = y_test2\n","submit2.to_csv('submit_lr2.csv',index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Top features with fine-tuned model\n","plt.figure(figsize=(16,7))\n","s1 = pd.Series(np.transpose(lr.coef_[0]), index=X_train2.columns).sort_values(ascending=False)[:20]\n","s2 = pd.Series(np.transpose(lr.coef_[0]), index=X_train2.columns).sort_values()[:20]\n","plt.subplot(121)\n","sns.barplot(y=s1.index, x=s1)\n","plt.title('Top positive coefficients')\n","plt.subplot(122)\n","sns.barplot(y=s2.index, x=s2)\n","plt.title('Top negative coefficients')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Next, we inspect the tweets that predicted probability differs the most from target outcome"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Error analysis\n","y_hat = pipeline_grid.predict_proba(X_train2)[:,1]\n","checker = train.loc[:,['text','keyword','location','target']]\n","checker['pred_prob'] = y_hat\n","checker['error'] = np.abs(checker['target'] - checker['pred_prob'])\n","\n","# Top 50 mispredicted tweets\n","error50 = checker.sort_values('error', ascending=False).head(50)\n","error50 = error50.rename_axis('id').reset_index()\n","error50.target.value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["Among the top 50 mispredicted tweets, only 4 are false positive"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pd.options.display.max_colwidth = 200\n","\n","error50.loc[0:10,['text','target','pred_prob']]"]},{"cell_type":"markdown","metadata":{},"source":["Reading the tweet texts, it is quite strange that they are labelled as disaster tweets indeed.\n","\n","That's it for now. Stay tuned for more analysis!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
